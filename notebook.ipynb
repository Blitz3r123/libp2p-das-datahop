{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def merge_nonbuilder_latencies(latency_files, node_type):\n",
    "    \"\"\"\n",
    "    Merge all validator latency files into one dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    latency_dfs = []\n",
    "    for latency_file in latency_files:\n",
    "        latency_df = pd.read_csv(latency_file)\n",
    "        latency_df['filename'] = latency_file.replace(\"_latency_stats_\" +node_type+ \".csv\", \"\")\n",
    "        latency_df[\"node_type\"] = node_type\n",
    "        latency_df['block_id'] = range(len(latency_df))\n",
    "        latency_df = latency_df[ [\"node_type\", \"filename\", \"block_id\", \"Total Sampling Latency (us)\"] ]\n",
    "        \n",
    "        latency_dfs.append(latency_df)\n",
    "\n",
    "    return pd.concat(latency_dfs)\n",
    "\n",
    "def get_nonbuilder_df(latency_files, node_type):\n",
    "    latency_df = merge_nonbuilder_latencies(latency_files, node_type)\n",
    "    latency_df['Latency (s)'] = latency_df['Total Sampling Latency (us)'] / 1_000_000\n",
    "    latency_df = latency_df[ [\"node_type\", \"filename\", \"block_id\", \"Latency (s)\"] ]\n",
    "\n",
    "    return latency_df\n",
    "\n",
    "def get_latency_files(node_type):\n",
    "    if node_type == \"builder\":\n",
    "        return [file for file in os.listdir() if file.endswith('builder.csv') and \"latency_stats\" in file.lower()][0]\n",
    "    else:\n",
    "        return [file for file in os.listdir() if file.endswith('_' +node_type+ '.csv') and \"latency_stats\" in file.lower()]\n",
    "\n",
    "def get_total_putgets(node_id, node_type):\n",
    "    total_stats_files = [file for file in os.listdir() if node_id in file and 'total_stats' in file.lower()]\n",
    "    \n",
    "    total_stats_file = total_stats_files[0]\n",
    "\n",
    "    total_stats_df = pd.read_csv(total_stats_file)\n",
    "\n",
    "    if node_type == \"builder\":\n",
    "        total_putgets = total_stats_df['Total PUT messages'][0]\n",
    "        failed_putgets = total_stats_df['Total failed PUTs'][0]\n",
    "    else:\n",
    "        total_putgets = total_stats_df['Total GET messages'][0]\n",
    "        failed_putgets = total_stats_df['Total failed GETs'][0]\n",
    "\n",
    "    return total_putgets, failed_putgets\n",
    "\n",
    "def get_total_stats(latency_df):\n",
    "    for index, row in latency_df.iterrows():\n",
    "        total_putgets, failed_putgets = get_total_putgets(row['filename'], row['node_type'])\n",
    "        success_putgets = total_putgets - failed_putgets\n",
    "        latency_df.loc[index, 'Total PUT/GETs'] = total_putgets\n",
    "        latency_df.loc[index, 'Successful PUT/GETs'] = success_putgets\n",
    "\n",
    "    return latency_df\n",
    "\n",
    "def get_operations():\n",
    "    # Get the operation files for each node\n",
    "    operation_files = [file for file in os.listdir() if file.endswith(\".csv\") and \"operations\" in file.lower() and \"putget_operations.csv\" not in file.lower()]\n",
    "    \n",
    "    operations_df = pd.DataFrame()\n",
    "\n",
    "    for operation_file in operation_files:\n",
    "        operation_df = pd.read_csv(operation_file)\n",
    "        operation_df['node_id'] = operation_file.split(\"_\")[0]\n",
    "        operation_df['node_type'] = operation_file.split(\"_\")[2].replace(\".csv\", \"\")\n",
    "        operation_df['timestamps'] = pd.to_datetime(operation_df['PUT timestamps'].fillna(operation_df['GET timestamps']), format='%H:%M:%S.%f')\n",
    "        operation_df['latencies_us'] = operation_df['PUT latencies'].fillna(operation_df['GET latencies'])\n",
    "        operation_df['Block ID'] = operation_df['Block ID'] + 1\n",
    "        operation_df[\"block_parcel_id\"] = (operation_df['Block ID'] * 1_000_000_000) + operation_df['Parcel ID']\n",
    "\n",
    "\n",
    "        operations_df = pd.concat([operations_df, operation_df])\n",
    "\n",
    "    # Find the minimum timestamp for each node_type\n",
    "    min_timestamps = operations_df.groupby('node_type')['timestamps'].min()\n",
    "\n",
    "    # Calculate the time offset for each node_type to synchronize them\n",
    "    offsets = min_timestamps - min_timestamps.min()\n",
    "\n",
    "    # Apply the time offset to synchronize timestamps\n",
    "    operations_df['synced_timestamps'] = operations_df['timestamps']\n",
    "    operations_df['synced_timestamps'] -= operations_df['node_type'].map(offsets)\n",
    "\n",
    "    # Convert the synchronized timestamps back to the original format\n",
    "    operations_df['synced_timestamps'] = operations_df['synced_timestamps'].dt.strftime('%H:%M:%S.%f')\n",
    "\n",
    "    # Delete the PUT/GET timestamps and latencies columns\n",
    "    operations_df = operations_df.drop(columns=['PUT timestamps', 'PUT latencies', 'GET timestamps', 'GET latencies', 'GET hops'])\n",
    "\n",
    "    row_count_before = len(operations_df)\n",
    "    # Remove any rows with NaN values\n",
    "    operations_df = operations_df.dropna()\n",
    "    row_count_after = len(operations_df)\n",
    "    print(\"Removed \" + str(row_count_before - row_count_after) + \" rows with NaN values from operations_df\")\n",
    "\n",
    "    return operations_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 rows with NaN values from operations_df\n"
     ]
    }
   ],
   "source": [
    "\n",
    "builder_latency_file = get_latency_files(\"builder\")\n",
    "validator_latency_files = get_latency_files(\"validator\")\n",
    "non_validator_latency_files = get_latency_files(\"nonvalidator\")\n",
    "\n",
    "validator_latency_df = get_nonbuilder_df(validator_latency_files, \"validator\")\n",
    "non_validator_latency_df = get_nonbuilder_df(non_validator_latency_files, \"nonvalidator\")\n",
    "\n",
    "validator_latency_df = get_total_stats(validator_latency_df)\n",
    "non_validator_latency_df = get_total_stats(non_validator_latency_df)\n",
    "\n",
    "# Get builder latency stats\n",
    "builder_latency_df = pd.read_csv(builder_latency_file)\n",
    "builder_latency_df['Latency (s)'] = builder_latency_df['Seeding Latency (us)'] / 1_000_000\n",
    "builder_latency_df['filename'] = builder_latency_file.replace(\"_latency_stats_builder.csv\", \"\")\n",
    "builder_latency_df['block_id'] = range(len(builder_latency_df))\n",
    "builder_latency_df[\"node_type\"] = \"builder\"\n",
    "builder_latency_df = builder_latency_df[ [\"node_type\", \"filename\", \"block_id\", \"Latency (s)\"] ]\n",
    "builder_latency_df = get_total_stats(builder_latency_df)\n",
    "\n",
    "# Merge all latency stats\n",
    "latency_df = pd.concat([builder_latency_df, validator_latency_df, non_validator_latency_df])\n",
    "latency_df['node_label'] = latency_df['node_type'] + \" \" + latency_df['filename'].apply(lambda x: x[:5])\n",
    "latency_df['Successful PUT/GETs'] = latency_df['Successful PUT/GETs'].astype(int)\n",
    "latency_df['Total PUT/GETs'] = latency_df['Total PUT/GETs'].astype(int)\n",
    "latency_df['Successful/Total Label'] = latency_df['Successful PUT/GETs'].astype(str) + '/' + latency_df['Total PUT/GETs'].astype(str)\n",
    "latency_df['Percentage'] = (latency_df['Successful PUT/GETs'] / latency_df['Total PUT/GETs'] * 100).round().astype(int)\n",
    "latency_df['node_label'] = latency_df['node_label'] + \" \" + latency_df['Successful/Total Label'].astype(str) + \" \" + latency_df['Percentage'].astype(str) + \"%\"\n",
    "\n",
    "# Remove first and last block\n",
    "latency_df = latency_df[ (latency_df['block_id'] != 0) & (latency_df['block_id'] != len(builder_latency_df)) ]\n",
    "\n",
    "putget_operations_df = get_operations()\n",
    "# putget_operations_df.to_csv(\"putget_operations.csv\", index=False)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "block_ids = putget_operations_df['Block ID'].unique()\n",
    "fig, axes = plt.subplots( len(block_ids) + 1 , 1, figsize=(20, 4*len(block_ids)))\n",
    "putget_operations_df_grouped = putget_operations_df.groupby(\"Block ID\")\n",
    "\n",
    "for block_id, block_df in putget_operations_df_grouped:\n",
    "    block_id_index = block_id - 1\n",
    "    ax = axes[block_id_index]\n",
    "    sns.scatterplot(x=\"synced_timestamps\", y=\"Parcel ID\", hue=\"node_type\", data=block_df, ax=ax, )\n",
    "\n",
    "    # Plot the starting signal vertical lines\n",
    "    starting_signal_df = block_df[block_df['Parcel ID'] == -1]\n",
    "    for index, row in starting_signal_df.iterrows():\n",
    "        if row['node_type'] == \"builder\":\n",
    "            ax.axvline(x=row['synced_timestamps'], color='green', linestyle='--')\n",
    "        elif row['node_type'] == \"validator\":\n",
    "            ax.axvline(x=row['synced_timestamps'], color='blue', linestyle='--')\n",
    "        elif row['node_type'] == \"nonvalidator\":\n",
    "            ax.axvline(x=row['synced_timestamps'], color='orange', linestyle='--')\n",
    "\n",
    "    ax.set_title(\"PUT/GET Operations for Block \" + str(block_id))\n",
    "    ax.set_xlabel(\"Increasing Time\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_ylabel(\"Parcel ID\")\n",
    "    ax.legend()\n",
    "\n",
    "i = len(block_ids)\n",
    "plt.title(\"Seeding/Sampling Latency for Each Block\")\n",
    "sns.barplot(x=\"block_id\", y=\"Latency (s)\", hue=\"node_label\", data=latency_df, ax=axes[i])\n",
    "axes[i].set_xlabel(\"Block Number\")\n",
    "axes[i].set_ylabel(\"Latency (s)\")\n",
    "axes[i].axhline(y=12, color='red', linestyle='--')\n",
    "axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
